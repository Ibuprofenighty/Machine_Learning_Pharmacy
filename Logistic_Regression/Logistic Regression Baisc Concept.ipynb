{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-approach",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "- **Two-Class Problems**: Logistic regression is intended for two-class or binary classification\n",
    "\n",
    "\n",
    "- **Unstable with Well-Separated Classes**: when the classes are well separated, the logistic regression becomes unstable\n",
    "\n",
    "\n",
    "- **Unstable with Few Examples**: it requires the number of examples much larger than number of features (else, overfitting)\n",
    "\n",
    "\n",
    "- **Assumes the Binomial Distribution of the Dependent Variable**\n",
    "\n",
    "\n",
    "- **Assumes the linearity between the Dependent Variable (Logit: ln[P/1-P])**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-leave",
   "metadata": {},
   "source": [
    "### Odds of Success\n",
    "**odds of success** as the probability of success divided by the probability of not success\n",
    "$$Odds \\space of \\space Success = \\frac{P}{1-P}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-gibson",
   "metadata": {},
   "source": [
    "### Logit\n",
    "$$Logit = ln(\\frac{P}{1-P}) = a_{0}(bias) + a_{1}\\cdot x_{1} + a_{2}\\cdot x_{2} + ... + a_{n}\\cdot x_{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-championship",
   "metadata": {},
   "source": [
    "### Cost Function (1): metric of tuning parameters\n",
    "\n",
    "**Maximal Likelihood Estimation (MLE)**: is a probabilistic framework for estimating the **parameters** of a model.\n",
    "\n",
    "In Maximum Likelihood Estimation, we wish to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters ($\\theta$). <br> Where X is, in fact, the joint probability distribution of all observations from the problem domain from 1 to n. \n",
    "$$P(X = x_{1}, x_{2}, x_{3}, â€¦, x_{n}; \\theta)$$\n",
    "\n",
    "\n",
    "Specifically, the choice of model and model parameters is referred to as a modeling hypothesis *h*, <br> and the problem  involves finding h that best explains the data *X*. <br> We can, therefore, find the modeling hypothesis that maximizes the likelihood function.\n",
    "$$maximize \\sum_{i}^{n} log(P(x_{i} ; h))$$\n",
    "\n",
    "$$MLE = max\\{\\sum_{i=1}^{n} [log(\\hat y_{i})\\cdot y_{i} +log(1-\\hat y_{i})\\cdot (1-y_{i})]\\}$$\n",
    "\n",
    "Calculating the negative of the log-likelihood function for the Bernoulli distribution is equivalent to calculating the cross-entropy function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
